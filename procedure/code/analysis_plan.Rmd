---
title: "Analysis Plan Version 1"
author: "Samuel Barnard and Matthew Mills"
date: "`r Sys.Date()`"
output: html_document
editor_options:
  markdown:
    wrap: sentence
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../../docs") })
nocite: '@*'
bibliography: "../../software.bib"
---

# Abstract

This study is a *reproduction* of: Tuholske, C., Lynch, V.D., Spriggs, R.
et al.
Hazardous heat exposure among incarcerated people in the United States.
*Nat Sustain 7*, 394–398 (2024).
<https://doi.org/10.1038/s41893-024-01293-y> [Git Repository](https://github.com/sparklabnyc/temperature_prisons_united_states_2024)

By reproducing the exploratory data analysis done in Tuholske et al. (2024), we seek to...

*1.* Identify the temporal resolution at which authors use population data to calculate population weighted hazardous heat days

*2.* Understand the population weighting mechanism in state-level hazardous heat calculations.

*3.* Evaluate the effectiveness of the author's methods compared to similar research.

# Study metadata

-   `Key words`: Comma-separated list of keywords (tags) for searchability. Geographers often use one or two keywords each for: theory, geographic context, and methods.
-   `Subject`: Social and Behavioral Sciences: Geography: Geographic Information Sciences, Human Geography, Nature and Society Relations
-   `Date created`: 04/14/25
-   `Date modified`: 04/21/25
-   `Spatial Coverage`: United States Lower 48
-   `Spatial Resolution`: Carceral facility points and States
-   `Spatial Reference System`: Specify the geographic or projected coordinate system for the study, e.g. EPSG:4326
-   `Temporal Coverage`: 1982-2020
-   `Temporal Resolution`: 1 year

## Original study spatio-temporal metadata

-   `Spatial Coverage`: United States Lower 48
-   `Spatial Resolution`: Carceral facility points and States
-   `Spatial Reference System`: spatial reference system of original study
-   `Temporal Coverage`: 1982-2020
-   `Temporal Resolution`: 1 year

# Study design

This study is a **reproduction study**, and the original study is more **exploratory** in design.
The primary objective for exploration through research and analysis investigated in the original study was exposure to hazardous heat in carceral facilities in the continental U.S.
The authors wanted to examine how exposure to hazardous heat changed over time from 1982-2020, as well as how exposure within carceral facilities compared to exposure in the rest of the state.
In general, determining the spatial distribution of carceral facilities with higher levels of hazardous heat exposure was also an objective of the original paper

# Materials and procedure

## Computational environment

### Original study computational environment

The original study data transformations and analysis were completed primarily in R using Rmd documents, as well as in Python.
The versions of R and Python used are not disclosed, but would have been R 4.3.3 or earlier, and Python 3.12 or earlier.

In the original study, R packages are called in across different scripts.
However, it seems that the important ones for this study are:

```{r eval=FALSE}
original_study_packages <- c(
  "dplyr", 
  "data.table", 
  "maptools",
  "mapproj",
  "rgeos",
  "rgdal",
  "RColorBrewer",
  "ggplot2",
  "raster", # planned deviation: we will be using `stars` in our reproduction
  "sp", # planned deviation: we will be using `sf` in our reproduction
  "plyr",
  "graticule",
  "zoo",
  "purrr",
  "cowplot",
  "janitor"
  )
```

### Prepare reproduction computational environment

For the reproduction study, we will be using R version 4.4.2, and the `groundhog` package to maintain package consistency.
All packages used will be up to date as of 2025-02-01.

We plan on using the packages `tidyverse`, `here`, `markdown`, `htmltools`, `dplyr`, `sf`, and `stars`.
As we encounter the need for other packages in our implementation of the code, we will make note of them as unplanned deviations.

```{r environment-setup, include = FALSE}
# record all the packages you are using here
# this includes any calls to library(), require(),
# and double colons such as here::i_am()
packages <- c("tidyverse", "here", "markdown", "htmltools", "dplyr", "sf", "stars", "tigris", "tmap", "dplyr", "data.table", "maptools", "mapproj", "rgeos", "rgdal", "RColorBrewer", "ggplot2", "graticule", "zoo", "purrr", "cowplot", "janitor")

# force all conflicts to become errors
# if you load dplyr and use filter(), R has to guess whether you mean dplyr::filter() or stats::filter()
# the conflicted package forces you to be explicit about this
# disable at your own peril
# https://conflicted.r-lib.org/
require(conflicted)

# load and install required packages
# https://groundhogr.com/
if (!require(groundhog)) {
  install.packages("groundhog")
  require(groundhog)
}

# this date will be used to determine the versions of R and your packages
# it is best practice to keep R and its packages up to date
groundhog.day <- "2025-02-01"

# this replaces any library() or require() calls
groundhog.library(packages, groundhog.day,
                  tolerate.R.version='4.4.2')
# you may need to install a correct version of R
# you may need to respond OK in the console to permit groundhog to install packages
# you may need to restart R and rerun this code to load installed packages
# In RStudio, restart r with Session -> Restart Session

# record the R processing environment
# alternatively, use devtools::session_info() for better results
writeLines(
  capture.output(sessionInfo()),
  here("procedure", "environment", paste0("r-environment-", Sys.Date(), ".txt"))
)

# save package citations
knitr::write_bib(c(packages, "base"), file = here("software.bib"))

# set up default knitr parameters
# https://yihui.org/knitr/options/
knitr::opts_chunk$set(
  echo = FALSE, # Show outputs, but not code. Change to TRUE to show code as well
  fig.retina = 4,
  fig.width = 8,
  fig.path = paste0(here("results", "figures"), "/")
)
```

## Data and variables

We are going to use data from the original study's git repository (linked on top level readme).
This includes:

\- Population data for the study period

\- Prison boundary polygons with facility information

\- State polygons

\- WBGT data, at prison point and state levels

### Population Data

```{r}
includeMarkdown(
  here("data", "metadata", "metadata_population.md")
)
```

### Prison Boundaries

```{r}
includeMarkdown(
  here("data", "metadata", "metadata_Prison_Boundaries.md")
)
```

### State Boundaries

```{r}
includeMarkdown(
  here("data", "metadata", "metadata_statesshp.md")
)
```

### WBGT Data

#### WBGT Data - Prison Level

```{r}
includeMarkdown(
  here("data", "metadata", "metadata_wbgt_prison.md")
)
```

#### WBGT Data - State Level

```{r}
includeMarkdown(
  here("data", "metadata", "metadata_wbgt_state.md")
)
```

## Prior observations

At the time of this pre-analysis plan, we have the derived data from the original study to work off of, and examined some of the csv tables.
We have neither visualized nor analyzed prison data or WBGTmax temperature data before.

## Bias and threats to validity

*There are no statistical tests in this study, so issues such as spatial heterogeneity/anisotropy/autocorrelation do not matter.*
Scale could be a threat to validity, because county populations are aggregated to calculate the number of population-weighted heat days in each state.
There is also a scale issue measuring micro-climate conditions at prison boundaries compared to 4 km temperature data.
Further, there is no specification of how heat days are calculated within each county given that counties do no map neatly to 4 km by 4km grids used to calculate hazardous heat days.
The ways in which the county boundaries are drawn also supports the argument that there is a Modifiable Area Unit Problem.

Both the scale and boundary issues also have a temporal component that may create threats to validity.

## Data transformations

### Planned deviation:

We will not attempt to produce the original study's WBGTmax grid because the methods are unclear, and therefore we will skip to joining the author-provided WBGTmax by day grid data to the prison points.

(When implementing plan) Explain what we believe the authors did to produce the WBGTmax grid and preliminary steps

### Transform data to create Figures 1a, 1b

(More descriptive segment of original study's workflow)

# OFFICE HOURS NOTES
recount the hazardous heat days (summarize hazardous heat excedence) per year by prison
# heat excedence by prison and year
#NOTES- sequencing is unclear, first round of regressions is done in data rangling, HAVE TO READ ALL OF THE CODE AND SUPPLEMENTARY MATRIALS AND THE PAPER 2b is in the wrangling, 2c is calcued in the actual figure 2 code

#### Step 1: Join author-provided WBGTmax by day grid data to author-provided carceral facility point data

#### Define years and data paths

```{r}
# Define years
start_year_wbgt <- 1982
end_year_wbgt <- 2020
years_wbgtmax <- c(start_year_wbgt:end_year_wbgt)

# Define data
raw_wbgt_prison_data <- here("data", "raw", "public", "wbgt_raw", "prison", "")
raw_wbgt_state_data <- here("data", "raw", "public", "wbgt_raw", "state", "")
```

*Unplanned Deviation:* Read in WBGTmax by prison data as single RDS frame.

(This step takes a while to run!)

Code is adapted from original repository

```{r eval=FALSE}
# Create data frame

dat.all = data.frame()
for(year in years_wbgtmax){
  print(paste0('Loading ',year))
  dat.current = readRDS(paste0(raw_wbgt_prison_data,'weighted_area_raster_prison_wbgtmax_daily_',year,'.rds'))
  dat.all= data.table::rbindlist(list(dat.all,dat.current))
  print(paste0('Loaded ',year))
}

saveRDS(dat.all,paste0(
  here("data", "derived", "private", ""), # quotation marked added based on assumption, moved file manually
  'weighted_area_raster_prison_wbgtmax_daily_',start_year_wbgt,'_',end_year_wbgt,'.rds'))

```

*Unplanned Deviation:* Data still needs to be joined to prison points

*Result:* Rds table of WBGTmax by day by prison

#### Step 2: Filter result by days when WBGTmax exceeded 28 degrees C

```{r eval=FALSE}
# read in WBGT prisons file
wbgtmax_prison <- readRDS(here("data", "derived", "private", "weighted_area_raster_prison_wbgtmax_daily_1982_2020.rds"))

# summarize by different temp. thresholds
dat.wbgt.summarised = wbgtmax_prison %>%
  mutate(wbgt_26=ifelse(wbgtmax>26,1,0),wbgt_28=ifelse(wbgtmax>28,1,0),
         wbgt_30=ifelse(wbgtmax>30,1,0),wbgt_35=ifelse(wbgtmax>35,1,0)) %>%
  group_by(prison_id, year) %>%
  dplyr::summarise(wbgt_26=sum(wbgt_26), wbgt_28=sum(wbgt_28), wbgt_30=sum(wbgt_30), wbgt_35=sum(wbgt_35))

saveRDS(dat.wbgt.summarised,here("data", "derived", "private", "dat.wbgt.summarised.rds"))
```

#### *begin here to avoid running first two chunks!* Step 3: Group by carceral facility type and year
The group by is already included in the study's filter/summary code from 02 --MM
Count to produce summary of days exceeded per year by facility

*Result:* Rds table with variables\
- prison facility

\- facility type

\- prison population

\- n days exceeding 28 degrees

\- year

*Unplanned Deviation:*
**Taken from original code
Take number of days in first year, last year and last 5 years of data
prepares data for figure 1 and figure 2b (and a little bit for 2c) --MM

```{r}
dat.wbgt.summarised <- readRDS(here("data", "derived", "private", "dat.wbgt.summarised.rds"))

dat.wbgt.summarised.start = dat.wbgt.summarised %>%
  dplyr::filter(year%in%c(start_year_wbgt)) %>%
  dplyr::select(-year) %>%
  dplyr::rename_with(~ paste0(., "_", start_year_wbgt), -prison_id)

dat.wbgt.summarised.end = dat.wbgt.summarised %>%
  dplyr::filter(year%in%c(end_year_wbgt)) %>%
  dplyr::select(-year) %>%
  dplyr::rename_with(~ paste0(., "_", end_year_wbgt), -prison_id)

dat.wbgt.summarised.last.5 = dat.wbgt.summarised %>%
  dplyr::filter(year%in%c((end_year_wbgt-4):end_year_wbgt)) %>%
  group_by(prison_id) %>%
  summarise(wbgt_26=mean(wbgt_26),
            wbgt_28=mean(wbgt_28),
            wbgt_30=mean(wbgt_30),
            wbgt_35=mean(wbgt_35)) %>%
  dplyr::rename_with(~ paste0(., "_", as.numeric(end_year_wbgt-4), "_", end_year_wbgt), -prison_id)
```

Trends in growth of number of day per year over time

Totals are calculated by multiplying the regression slope by the total number of years -MM

```{r}
dat.wbgt.summarised.regression.26 = dat.wbgt.summarised %>%
  mutate(year=as.numeric(year)) %>%
  group_by(prison_id) %>%
  do(broom::tidy(lm(wbgt_26 ~ year, data = .))) %>%
  dplyr::filter(term=='year') %>%
  select(prison_id,estimate) %>%
  mutate(total_change_wbgt_26=estimate*(length(years_wbgtmax))) %>%
  rename(annual_change_wbgt_26=estimate) %>%
  select(prison_id,annual_change_wbgt_26,total_change_wbgt_26)

dat.wbgt.summarised.regression.28 = dat.wbgt.summarised %>%
  mutate(year=as.numeric(year)) %>%
  group_by(prison_id) %>%
  do(broom::tidy(lm(wbgt_28 ~ year, data = .))) %>%
  dplyr::filter(term=='year') %>%
  select(prison_id,estimate) %>%
  mutate(total_change_wbgt_28=estimate*(length(years_wbgtmax))) %>%
  rename(annual_change_wbgt_28=estimate) %>%
  select(prison_id,annual_change_wbgt_28,total_change_wbgt_28)

dat.wbgt.summarised.regression.30 = dat.wbgt.summarised %>%
  mutate(year=as.numeric(year)) %>%
  group_by(prison_id) %>%
  do(broom::tidy(lm(wbgt_30 ~ year, data = .))) %>%
  dplyr::filter(term=='year') %>%
  select(prison_id,estimate) %>%
  mutate(total_change_wbgt_30=estimate*(length(years_wbgtmax))) %>%
  rename(annual_change_wbgt_30=estimate) %>%
  select(prison_id,annual_change_wbgt_30,total_change_wbgt_30)

dat.wbgt.summarised.regression.35 = dat.wbgt.summarised %>%
  mutate(year=as.numeric(year)) %>%
  group_by(prison_id) %>%
  do(broom::tidy(lm(wbgt_35 ~ year, data = .))) %>%
  dplyr::filter(term=='year') %>%
  select(prison_id,estimate) %>%
  mutate(total_change_wbgt_35=estimate*(length(years_wbgtmax))) %>%
  rename(annual_change_wbgt_35=estimate) %>%
  select(prison_id,annual_change_wbgt_35,total_change_wbgt_35)
```

##### Merge regression analyses into one data frame\

```{r}
dat.wbgt.summarised.regression.merged = left_join(dat.wbgt.summarised.start,dat.wbgt.summarised.end) %>%
  left_join(.,dat.wbgt.summarised.last.5) %>%
  left_join(.,dat.wbgt.summarised.regression.26) %>%
  left_join(.,dat.wbgt.summarised.regression.28) %>%
  left_join(.,dat.wbgt.summarised.regression.30) %>%
  left_join(.,dat.wbgt.summarised.regression.35)
```

##### Save regression file

```{r}
saveRDS(dat.wbgt.summarised,here("data", "derived", "private", "dat.wbgt_regression_analysis.rds"))
```

#### Step 4: combine state level WBGT data into a single file

##### Iterate across years of WBGT data and combine into one large file

```{r}
dat.all = data.frame()
for(year in years_wbgtmax){
  print(paste0('Loading ',year))
  dat.current = readRDS(paste0(raw_wbgt_state_data,'weighted_area_raster_fips_wbgtmax_daily_',year,'.rds'))
  dat.all= data.table::rbindlist(list(dat.all,dat.current))
  print(paste0('Loaded ',year))
}

saveRDS(dat.all,paste0(
  here("data", "derived", "private", ""), # quotation marked added based on assumption, moved file manually
  'weighted_area_raster_state_wbgtmax_daily_',start_year_wbgt,'_',end_year_wbgt,'.rds'))
```

#### Step 4b: summarize hazardous heat day count by county

```{r}
wbgtmax_state <- readRDS(here("data", "derived", "private", "weighted_area_raster_state_wbgtmax_daily_1982_2020.rds"))

# code from original study
dat.wbgt.state.summarised = wbgtmax_state %>%
  mutate(wbgt_26=ifelse(wbgtmax>26,1,0),wbgt_28=ifelse(wbgtmax>28,1,0),
         wbgt_30=ifelse(wbgtmax>30,1,0),wbgt_35=ifelse(wbgtmax>35,1,0)) %>%
  group_by(fips, year) %>%
  dplyr::summarise(wbgt_26=sum(wbgt_26), wbgt_28=sum(wbgt_28), wbgt_30=sum(wbgt_30), wbgt_35=sum(wbgt_35)) %>%
  mutate(fips=as.character(fips))

saveRDS(dat.wbgt.state.summarised,here("data", "derived", "private", "dat.wbgt.state.summarised.rds"))
```

*Unplanned Deviation:*
Per original study, fix counties to be consistent all the way through

```{r}
dat.wbgt.state.summarised = dat.wbgt.state.summarised %>%
      mutate(fips = case_when(
        fips== '08001' | fips== '08013' | fips== '08059' | fips== '08123' ~ '08014', # 08001, 08013, 08059, 08123 -> 08014
        fips== '12025' ~ '12086', #  12025 -> 12086
        fips== '30031' | fips== '30067'~ '30113', # 30113 -> 30031, 30067
        fips== '46113' ~ '46102', # 46113 -> 46102
        fips== '51560' ~ '51005', # 51560 -> 51005
        fips== '51780' ~ '51083', # 51780 -> 51083
        fips== '51515' ~ '51019', # 51515 -> 51019
        TRUE ~ fips
        ))
```

Next, summarize by the fixed counties

```{r}
dat.wbgt.state.summarised = dat.wbgt.state.summarised %>%
  group_by(fips, year) %>%
  dplyr::summarise(wbgt_26=mean(wbgt_26),wbgt_28=mean(wbgt_28),wbgt_30=mean(wbgt_30),wbgt_35=mean(wbgt_35))
```

#### Step 4c: load county populations

```{r}
# original study code
dat_pop= data.frame()
for(year_selected in years_wbgtmax){
  if(year_selected<1990){
    filename_in = paste0(here("data", "raw", "public", "population", "pre_1990", ""),
                         'pop_monthly_10_year_age_groups_', year_selected,'.csv')  
  }
  if(year_selected>=1990){
    filename_in = paste0(here("data", "raw", "public", "population", "vintage_2020", ""),
                         'pop_monthly_10_year_age_groups_',year_selected,'.csv')
  }
  dat_year=readr::read_csv(filename_in)
  dat_pop = data.table::rbindlist(list(dat_pop,dat_year))
  rm(dat_year)
}

dat_pop = dat_pop %>%
  dplyr::filter(month==6) %>%
  dplyr::group_by(year,fips) %>%
  dplyr::summarise(pop=sum(pop))
```

#### Step 4d: join population data with summarized, county-level hazardous heat data

```{r}
# original study code
dat.wbgt.state.summarised.merged = left_join(dat.wbgt.state.summarised,dat_pop)
dat.wbgt.state.summarised.merged.na = dat.wbgt.state.summarised.merged %>% dplyr::filter(is.na(pop)==TRUE)

saveRDS(dat.wbgt.state.summarised.merged,here("data", "derived", "private", "dat.wbgt.state.summarised.merged.rds"))
```

*Unplanned Deviation:* Report list of counties with NA population values.

```{r}
dat.wbgt.state.summarised.merged.na
```

Join to county shapefile and visualize ## NEED TO FIX

```{r}
counties <- tigris::counties()

counties48 <- counties |>
  dplyr::filter(
    as.numeric(STATEFP) != 02,
    as.numeric(STATEFP) != 15, 
    as.numeric(STATEFP) != 72, 
    as.numeric(STATEFP) != 78,
    as.numeric(STATEFP) != 66,
    as.numeric(STATEFP) != 69,
    as.numeric(STATEFP) != 60,
         )

na.counties <- counties48 |>
  left_join(
    dat.wbgt.state.summarised.merged.na, by = c("GEOID" = "fips")
  )

# visualize counties with missing population data
tmap_mode("plot")

na.counties.fixed <- na.counties |>
  mutate(missing = ifelse(year == 2012, "Missing Population Data", NA))

tm_shape(na.counties.fixed) + tm_polygons(
  fill = "missing",
  fill.scale = tm_scale_categorical(),
  fill.legend = tm_legend(
    title = "2012 counties (lower 48)",
    na.show = FALSE
  )
)
```

Filter entire dataset by NA fips and see if they have population data
dplyr filter where fips is %IN% group of fips code

Take average of year before and year after
dplyr filter where fips is in group of fips code, and in year 2011 and 2013
group by average of pop

#### Step 4e: Take weighted average of each state in each year by population

```{r}
dat.wbgt.summarised.merged.weighted = dat.wbgt.state.summarised.merged %>%
  mutate(state=substr(fips,1,2)) %>%
  group_by(state,year) %>%
  dplyr::summarise(wbgt_26 = weighted.mean(wbgt_26,pop),
            wbgt_28 = weighted.mean(wbgt_28,pop),
            wbgt_30 = weighted.mean(wbgt_30,pop),
            wbgt_35 = weighted.mean(wbgt_35,pop))
```

Take weighted average of entire country in each year by population

```{r}
dat.wbgt.summarised.merged.weighted.national = dat.wbgt.state.summarised.merged %>%
  group_by(year) %>%
  dplyr::summarise(wbgt_26 = weighted.mean(wbgt_26,pop),
            wbgt_28 = weighted.mean(wbgt_28,pop),
            wbgt_30 = weighted.mean(wbgt_30,pop),
            wbgt_35 = weighted.mean(wbgt_35,pop)) %>%
  dplyr::mutate(state='nat') %>% # changed national code to nat since 99 is often American Samoa
  dplyr::relocate(state)
```

Combine state-specific with national

```{r}
dat.wbgt.summarised.merged.weighted = rbind(dat.wbgt.summarised.merged.weighted.national,dat.wbgt.summarised.merged.weighted)
```

## Save state file over time

```{r}
saveRDS(dat.wbgt.summarised.merged.weighted,paste0(here("data", "derived", "private", "weighted_area_raster_state_wbgtmax_daily_1982_2020_over_time.rds")))
```

## Analysis

### Analyze data to create Figures 2a, 2b and 2c

Figure 2b, 2c - results are based on linear regression models

#### Step 4: Begin with result from Step 3

#### Step 5: Spatial join county population by year data

*Result:* Rds table with variables\
- prison facility

\- facility type

\- prison population

\- county

\- population

\- n days exceeding 28 degrees

\- year

#### Step 6: Population-weighted aggregation

Aggregate data into states

Weighted sum of days exceeded across all counties of the state

Sum of days exceeded multiplied by (Ratio of county population / state population)

## Planned deviations for reproduction:

### Deviation 1: Investigating temperature threshold

Repeat workflow for reproducing figures 1 and 2, instead filtering for days when WBGTmax exceeded 29.4 degrees C (85 degrees F standard informed by other literature)

### Deviation 2: Investigating sources of uncertainty/error/bias

#### How many open facilities had a population of -999? (Potential source of uncertainty)


```{r}
# load prison boundaries shapefile



# XXX
dat.wbgt.summarised.regression = readRDS(here("data", "derived", "private", "dat.wbgt_regression_analysis.rds"))

```

#### Step 7: Select facilities with population of -999 from author-provided carceral facilities data.

Report number of facilities compared to authors' number.

# Results: Present figures (reproduced from original study and planned deviations).

## Create Fig. 1a and b using results from step 3

### Make Figure 1a

Work through author's code for Figure 1

#### Load WBGT regression file

```{r}
dat.wbgt.summarised.regression = readRDS(here("data", "derived", "private", "dat.wbgt_regression_analysis.rds"))
```

#### Prepare map structure (using ggplot)

```{r}
# for map theme to plot in ggplot
theme_map = function(base_size=10, base_family=""){
    require(grid)
    theme_bw(base_size=base_size,base_family=base_family) %+replace%
    theme(axis.line=element_blank(),
    axis.text=element_blank(),
    axis.ticks=element_blank(),
    axis.title=element_blank(),
    panel.background=element_blank(),
    panel.border=element_blank(),
    panel.grid=element_blank(),
    panel.margin=unit(0,"lines"),
    plot.background=element_blank(),
    legend.position = 'bottom'
    )
}
```

#### Load US prison shapefile to load details and mapping

(Stuck on this step!) Author's code doesn't work

```{r}
# load shapefile of entire United States by prison
#huh what happened here??
us.main = readRDS(dsn=here("data", "raw", "public", "shapefiles", "Prison_Boundaries", "Prison_Boundaries_Edited.shp"))
# THIS ONE WORKS?
us.main = st_read(dsn=here("data", "raw", "public", "shapefiles", "Prison_Boundaries"),
                  layer = "Prison_Boundaries_Edited")

us.main = st_transform(us.main, crs = ("+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs"))

# fortify to prepare for plotting in ggplot
map = fortify(us.main)

# extract data from shapefile

## Add an id column
us.main$id <- as.character(1:nrow(us.main))

## Extract data
shapefile.data <- st_drop_geometry(us.main)


# fix the weird Alabama mistake
shapefile.data = shapefile.data %>%
  dplyr::mutate(STATEFP=case_when(STATEFP=='10' & STATE=='AL'~ '01',
                               TRUE ~ STATEFP))


# merge selected data to map_create dataframe for colouring of ggplot
USA.df = merge(map, shapefile.data, by='FID')
USA.df$prison_id = as.integer(as.character(USA.df$FID))
```


Merge prison over time file with shapefile (for geometry or prison data?)
```{r}
# load prison wbgt over time file
dat.wbgt.summarised.regression = readRDS(here("data", "derived", "private", "weighted_area_raster_state_wbgtmax_daily_1982_2020_over_time.rds"))

# merge weighted prison values and shapefile data, group by, relevel factor, and recalculate at-risk population
dat.wbgt.summarised.merged.weighted.prison = left_join(dat.wbgt.summarised,shapefile.data, by=c('prison_id'='FID')) %>% 
  dplyr::filter(STATUS=='OPEN') %>% 
  dplyr::filter(POPULATION > 0) %>%
  dplyr::group_by(STATE,STATEFP,TYPE,year) %>%
  dplyr::summarise(wbgt_26 = weighted.mean(wbgt_26,POPULATION),
            wbgt_28 = weighted.mean(wbgt_28,POPULATION),
            wbgt_30 = weighted.mean(wbgt_30,POPULATION),
            wbgt_35 = weighted.mean(wbgt_35,POPULATION),
            POPULATION = sum(POPULATION)) %>%
  dplyr::mutate(STATE=factor(STATE, levels=rev(sort(unique(STATE))))) %>%
  dplyr::mutate(wbgt_26_pop = wbgt_26 * POPULATION,
                wbgt_28_pop = wbgt_28 * POPULATION,
                wbgt_30_pop = wbgt_30 * POPULATION,
                wbgt_35_pop = wbgt_35 * POPULATION)
```

calculate 5-year averages
```{r}
# most recent 5 years (2016-2020)
dat.wbgt.summarised.merged.weighted.prison.5year.average = dat.wbgt.summarised.merged.weighted.prison %>%
  dplyr::filter(year%in%years_wbgtmax[c((length(years_wbgtmax)-4):length(years_wbgtmax))]) %>%
  dplyr::group_by(STATE,STATEFP,TYPE) %>%
  dplyr::summarise(wbgt_26_mean = mean(wbgt_26),
                   wbgt_28_mean = mean(wbgt_28),
                   wbgt_30_mean = mean(wbgt_30),
                   wbgt_35_mean = mean(wbgt_35),
                   wbgt_26_pop_mean = mean(wbgt_26_pop),
                   wbgt_28_pop_mean = mean(wbgt_28_pop),
                   wbgt_30_pop_mean = mean(wbgt_30_pop),
                   wbgt_35_pop_mean = mean(wbgt_35_pop)) %>%
  dplyr::rename(Type=TYPE) %>%
  dplyr::mutate(Type=case_when(Type=='COUNTY' ~ 'County',
                               Type=='LOCAL' ~ 'Local',
                               Type=='FEDERAL' ~ 'Federal',
                               Type=='STATE' ~ 'State',
                               TRUE ~ 'Other')) %>%
  dplyr::mutate(Type=as.factor(Type)) %>%
  dplyr::mutate(Type=factor(Type,levels=c('Federal','State','County','Local','Other')))

# earliest 5 years (1982-1987)
dat.wbgt.summarised.merged.weighted.prison.first.5year.average = dat.wbgt.summarised.merged.weighted.prison %>%
  dplyr::filter(year%in%years_wbgtmax[c(1:5)]) %>%
  dplyr::group_by(STATE,STATEFP,TYPE) %>%
  dplyr::summarise(wbgt_26_mean = mean(wbgt_26),
                   wbgt_28_mean = mean(wbgt_28),
                   wbgt_30_mean = mean(wbgt_30),
                   wbgt_35_mean = mean(wbgt_35),
                   wbgt_26_pop_mean = mean(wbgt_26_pop),
                   wbgt_28_pop_mean = mean(wbgt_28_pop),
                   wbgt_30_pop_mean = mean(wbgt_30_pop),
                   wbgt_35_pop_mean = mean(wbgt_35_pop)) %>%
  dplyr::rename(Type=TYPE) %>%
  dplyr::mutate(Type=case_when(Type=='COUNTY' ~ 'County',
                               Type=='LOCAL' ~ 'Local',
                               Type=='FEDERAL' ~ 'Federal',
                               Type=='STATE' ~ 'State',
                               TRUE ~ 'Other')) %>%
  dplyr::mutate(Type=as.factor(Type)) %>%
  dplyr::mutate(Type=factor(Type,levels=c('Federal','State','County','Local','Other')))
```

## Create Figure 1a
```{r}
library(MetBrewer)

plot_bar_chart = function(threshold_chosen,legend_chosen=1){ 
  
  # calculate rank of bars
  dat.rank = dat.wbgt.summarised.merged.weighted.prison.5year.average %>%
    group_by(STATE) %>%
    dplyr::summarise(sum=sum(get(paste0('wbgt_',threshold_chosen,'_pop_mean')))) %>%
    dplyr::arrange(sum) %>%
    dplyr::mutate(rank=row_number())
  
  dat.plot = left_join(dat.wbgt.summarised.merged.weighted.prison.5year.average,dat.rank)

 p = ggplot() + 
   geom_bar(data=dat.plot, 
                aes(x=reorder(STATE, rank),
                              y=get(paste0('wbgt_',threshold_chosen,'_pop_mean')),fill=Type),stat = "identity") +
   ylab(paste0('Mean annual number of dangerous hot-humid person-days\nfor incarcerated people during ',(end_year_wbgt-4),'-',end_year_wbgt)) + 
   xlab('State') + 
   scale_y_continuous(label=scales::comma) + 
   coord_flip() +
   scale_fill_manual(values=met.brewer('Lakota')) + 
   theme_bw() + theme(text = element_text(size = 10), 
   panel.grid.major = element_blank(),axis.text.x = element_text(angle=0 , size=8, vjust=0.5), 
   plot.title = element_text(hjust = 0.5),panel.background = element_blank(), 
   panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), 
   panel.border = element_rect(colour = "black"),strip.background = element_blank(), 
   legend.position = 'bottom',legend.justification='center', 
   legend.background = element_rect(fill="white", linewidth=.5, linetype="dotted"))

 return(p) 
}

plot.bar.chart.26 = plot_bar_chart(26,1)

plot.bar.chart.28 = plot_bar_chart(28,1)

plot.bar.chart.30 = plot_bar_chart(30,1)
```

## Create 1b
```{r}
# load lower 48 US states
us = st_read(dsn=here("data", "raw", "public", "shapefiles", "states"),
                  layer = "states")
#us.main = readRDS(dsn=here("data", "raw", "public", "shapefiles", "Prison_Boundaries"),
#                  layer = "Prison_Boundaries_Edited")

# reproject shapefile
us_aea = st_transform(us, crs = ("+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs"))
us_aea = us
#us_aea@data$id = rownames(us_aea@data) #????

# extract coordinates
us_aea <- st_as_sf(us_aea, coords = c("long", "lat"))

# only keep mainland US States
#us_aea = us_aea[us_aea$STATE_FIPS %in% states_included,]
#us_aea = st_transform(us_aea, crs = ("+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs"))
us_aea <- us_aea %>%
  dplyr::filter(STATE_NAME != "Hawaii",
                STATE_NAME != "District of Columbia",
                STATE_NAME != "Alaska")

us_aea = fortify(us_aea)


## Preapare summary maps
# merge map with mean values
USA.df.merged = left_join(USA.df,dat.wbgt.summarised.regression.merged,by='prison_id') ##changed to merged file? fid??
USA.df.merged = with(USA.df.merged, USA.df.merged[order(FID),]) # I CHANGED THIS IDK IF I MESSED IT UP -MM

# map legends and colors
legend_create_count_start_year = function(threshold)(paste0('Number of dangerous hot-humid days\nin ',start_year_wbgt))
legend_create_count_end_year = function(threshold)(paste0('Number of dangerous hot-humid days\nin ',end_year_wbgt))
legend_create_count_last_5_years = function(threshold)(paste0('Mean annual number of\ndangerous hot-humid days during ',(end_year_wbgt-4),'-',end_year_wbgt))
legend_create_annual_change = function(threshold)(paste0('Annual change during\n',start_year_wbgt,'-',end_year_wbgt ))
legend_create_total_change = function(threshold)(paste0('Total change in annual number of dangerous hot-humid days\nduring ',start_year_wbgt,'-',end_year_wbgt))

# filter for open and populated prisons !!!!!
conflicts_prefer(dplyr::filter)
USA.df.merged = USA.df.merged %>% 
  dplyr::filter(STATUS.x=='OPEN') %>% filter(POPULATION.x > 0)
```

Plot count in last 5 years
```{r}
us_aea <- us_aea %>%
  mutate(long = st_coordinates(st_centroid(us_aea$geometry))[,1],
         lat = st_coordinates(st_centroid(us_aea$geometry))[,2])
USA.df.merged <- USA.df.merged %>%
  mutate(long = st_coordinates(st_centroid(USA.df.merged$geometry))[,1],
         lat = st_coordinates(st_centroid(USA.df.merged$geometry))[,2])


plot_last_5_years = function(threshold_chosen,legend_chosen=0){
  p = ggplot() +
    geom_polygon(data=us_aea,aes(x=long,y=lat,group=STATE_NAME),fill='grey35',color='black',size=0.5) +
    geom_polygon(data=USA.df.merged,aes(x=long,y=lat,group=FID,
                                        color=get(paste0('wbgt_',threshold_chosen,'_',(end_year_wbgt-4),'_',end_year_wbgt))),size=1.2) +
    scale_color_gradientn(colors=rev(MetBrewer::met.brewer('Tam')), limits = c(0,max(USA.df.merged$wbgt_28_2016_2020))) +
    coord_fixed() + xlab('') + ylab('') + theme_bw() +
    theme(panel.grid.major = element_blank(),
        axis.text.x = element_text(angle=90), axis.ticks.x=element_blank(),legend.text=element_text(size=5),
        panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"),
        panel.border = element_rect(colour = "black"),strip.background = element_blank(),legend.justification='center',
        legend.position = 'bottom', legend.background = element_rect(fill="white", size=.5, linetype="dotted")) +
    theme_map()
  
  if(legend_chosen==0){
    p = p +
          guides(color=FALSE)
  }
  if(legend_chosen==1){
    p = p +
          guides(color = guide_colorbar(direction = "horizontal", title.position="left",barwidth = 10,barheight = 1,title.vjust = 0.8, 
                                  title = legend_create_count_last_5_years(threshold_chosen)),legend.text=element_text(size=5)) 
    }  

    return(p)

}


plot.count.end.year.legend = cowplot::get_legend(plot_last_5_years(26,1))
```

```{r}
plot.count.last.5.years.26 = plot_last_5_years(26,1)
plot.count.last.5.years.28 = plot_last_5_years(28,1)
plot.count.last.5.years.30 = plot_last_5_years(30,1)
```

# Plot figures 1a and 1b
```{r}
library(cowplot)
pdf(paste0(here("data", "derived", "pubilc"),'Figure_1.pdf'),paper='a4r',width=0,height=0)
plot_grid(plot.bar.chart.28, plot.count.last.5.years.28, 
          label_size = 14 ,
          label_x = -0.02, label_y = 1,
          labels = c('(a)', '(b)'))
dev.off()


plot_grid(plot.bar.chart.28, plot.count.last.5.years.28, 
          label_size = 14 ,
          label_x = -0.02, label_y = 1,
          labels = c('(a)', '(b)'))


```

## Create Fig. 2a, b and c using results from step 6

## Map result from step 7 to examine data quality issue

# Discussion

The study showed important first steps towards participating in open source conventions. Including a linked github repository containing the study code and a supplementary materials document improved the study’s legibility. However, these documents could be improved to ensure better reproducibility and open access.

Key barriers to reproducibility identified were:
1. *Brief Methods Section* - The methods section of the published paper does not explain key information about the data transformation and analysis process, selection of prison facilities to include in the study, or the extent of the study and choice to remove Alaska, Hawaii, and non-state US territories.
2. *Missing Climate Data* - The github repository does not include raw PRISM data used to calculate WBGT max, any form of the 4x4km climate raster, or a download script to retrieve data. It is therefore impossible to reproduce their climate analysis or determine any methods used to resolve boundary issues.
3. *Raster to Polygon Join* - The methods section and repository do not include a method for joining the raster data to the prison and county geometries. Therefore, it is unclear what method was used to resolve edge cases.
4. *Prison Population Metadata* - The study does not specify the date the prison data were downloaded or how the prison boundaries file was edited.
5. *Repository Organization and Labeling* - Mislabeled and extraneous files in the repository make it difficult to navigate. Many files throughout the repository are named “raster” but do not include raster data.
6. *Code Documentation* - Code files could be more explicitly and thoroughly documented for readability. For example, the README.txt file included in the python folder outlines a different series of code files than is present in the repository. Inclusion of intermediate results including attribute counts would improve reproducibility. Without a clear workflow in the methods section, the data transformation process remains opaque.
7. *Packages* - The use of a variety of packages, including `rgdal` which is no longer operational, impedes reproducibility. Standardization of open source package use and the groundhog package that stores and loads dates and versions for all packages would improve reproducibility.

Key threats to validity identified were:
1. *Barriers to Reproducibility* - The study is not reproducible which presents a threat to validity because its results cannot be verified.
2. *WBGTmax Raster* - Hazardous heat exposure is calculated using a 4x4km grid of daytime highs to county populations, presenting scale and boundary issues.
  a. *Scale* - Both of these grid and county geometries are large enough to house significant variation in heat and population density which could skew results, especially in densely populated areas.
  b. *Boundary* - The lack of a documented method to resolve boundary issues joining the raster to prison and county geometries (e.g. centroid, area of overlap, area-weighted reaggregation etc.) presents a threat to the validity of the hazardous heat exposure and relative risk calculations.
  c. *Heat Risk Metric* - Recent studies of heat risk have indicated that nighttime lows predict heat related mortality. When nighttime lows are high, the body is unable to recover from heat stress, exacerbating health risks.\*
3. *Prison Data* - The study and its repository consistently refer to carceral facilities as “prisons” despite also analyzing data from jails, detention centers, and other facilities. Missing data and documentation and the temporal resolution threaten validity.
  a. *Missing Population Data* - While the study focuses on the 4078 open and populated facilities, the HIFLD prison data file contains data on 6738 facilities. 1817 of these facilities reported being open but did not have available population data. 12 reported a population greater than 0 but did not have available data on their status (open or closed). 80 facilities did not have available status or population data.
  b. *Temporal Resolution* - The study uses carceral facility population data from a single, unspecified date to calculate changes in heat risk over 38 years. This threatens validity because these populations are not stable and the lack of a specific date makes it impossible to check biases.
4. *Selective Reporting Bias* - While the supplementary materials and github repository included additional figures and analyses testing different temperature thresholds and data visualizations, there is minimal documentation of the reasoning used to select the figures in the published study.

>\*Murage P, Hajat S, Kovats RS. Effect of night-time temperatures on cause and age-specific mortality in London. Environ Epidemiol. 2017 Dec;1(2):e005. doi: 10.1097/EE9.0000000000000005. Epub 2017 Dec 13. PMID: 33195962; PMCID: PMC7608908.

Through this reproduction in progress, we were able to identify key barriers to reproducibility and threats to validity in the study. The reproduction was limited by the time and energy of the reproducers (Sam and Matthew) and our limited expertise handling large datasets containing raster data and time series data. However, our inability to reproduce or verify all of the figures indicates that reproducibility attempts in the study were not sufficient to reduce barriers. Producing a more thorough reproduction could be accomplished by:
Recreating Figure 2
Mapping out the authors’ workflow and study design based on the code
Reanalyzing and visualizing the data using open source packages
Visualizing and investigating prisons with missing data
Replicating the study using nighttime low temperatures

Ultimately, the study provides a template for exploring general trends in extreme heat risk for incarcerated populations. With further attention to clarity, accessibility, and reproducibility, this study and its reproductions could contribute to a reporting or monitoring system for heat risk in carceral facilities. Despite threats to validity and reproducibility, the study provides a basis for critical future research in environmental hazards and abolition.


# Integrity Statement

This is the first version of our pre-analysis plan.
Any deviations in our workflow will be documented as unplanned deviations.

# Acknowledgements

This report is based upon the template for Reproducible and Replicable Research in Human-Environment and Geographical Sciences, [DOI:[10.17605/OSF.IO/W29MQ](DOI:%5B10.17605/OSF.IO/W29MQ){.uri}](https://doi.org/10.17605/OSF.IO/W29MQ)

## References

> Kedron, P., & Holler, J.
> (2023).
> Template for Reproducible and Replicable Research in Human-Environment and Geographical Sciences.
> <https://doi.org/10.17605/OSF.IO/W29MQ>

